import os
import json
import uuid
import argparse
import requests

from io import BytesIO
from pathlib import Path

import boto3
import numpy as np
import faiss    # pip install faiss-cpu
import cv2
from PIL import Image, ImageOps, ImageFile, ImageFilter

from flask import Flask, request, jsonify
from flask_cors import CORS

from sqlalchemy import create_engine, Column, Integer, String, DateTime # DateTime ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from sqlalchemy.orm import sessionmaker, declarative_base
from sqlalchemy.sql import func # func ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ç”¨)


# â”€â”€ å…±é€šè¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# DB
DATABASE_URL = os.environ.get(
    "DATABASE_URL",
    "sqlite:///./local_dev.db"
)
engine  = create_engine(DATABASE_URL)
Session = sessionmaker(bind=engine)
Base    = declarative_base()

class ProductMapping(Base):
    __tablename__ = "products"
    id     = Column(Integer, primary_key=True)
    name   = Column(String)
    s3_key = Column(String)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), server_onupdate=func.now())



# S3 ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
ImageFile.LOAD_TRUNCATED_IMAGES = True
S3_BUCKET = os.environ.get("S3_BUCKET", "registered_images")
s3        = boto3.client("s3") # ã‚°ãƒ­ãƒ¼ãƒãƒ«S3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ


# ç‰¹å¾´é‡è¨­å®š (ã‚°ãƒ­ãƒ¼ãƒãƒ«å®šæ•°)
FEATURE_DIM = 256
SIFT_SIGMA = 1.6


# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç½®ãå ´
CACHE_DIR  = "cache"
INDEX_PATH = os.path.join(CACHE_DIR, "faiss_v2.index") # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã®ãŸã‚åå‰å¤‰æ›´ã‚‚æ¤œè¨
KEYS_PATH  = os.path.join(CACHE_DIR, "keys.json")


# Flask
app = Flask(__name__)
CORS(app)
app.logger.setLevel("INFO")


# âœ… ã“ã“ã«è¿½è¨˜ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä½œæˆï¼‰
Base.metadata.create_all(bind=engine)


# ğŸ”½ ã“ã“ã«è¿½è¨˜ï¼ ğŸ”½
@app.route("/build_cache", methods=["POST"])
def trigger_build_cache():
    try:
        build_cache() # ã‚°ãƒ­ãƒ¼ãƒãƒ« FEATURE_DIM ã‚’ä½¿ç”¨
        return jsonify({"status": "ok", "message": "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å†æ§‹ç¯‰ã—ã¾ã—ãŸ"}), 200
    except Exception as e:
        app.logger.exception("ã‚­ãƒ£ãƒƒã‚·ãƒ¥å†æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼")
        return jsonify({"status": "error", "message": str(e)}), 500


@app.route("/register_image_v2", methods=["POST"])
def register_image_v2():
    try:
        data = request.get_json()
        image_url = data.get("image_url")
        name = data.get("name")

        if not image_url or not name:
            return jsonify({"message": "image_url or name missing", "status": "error"}), 400

        # S3 ã‹ã‚‰ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        response = requests.get(image_url)
        response.raise_for_status() # HTTPã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
        img_pil = Image.open(BytesIO(response.content))
        
        # ä¸€è²«ã—ãŸå‰å‡¦ç†
        processed_img_pil = preprocess_pil(img_pil)

        desc = extract_sift(processed_img_pil) # dim ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ« FEATURE_DIM ã‚’ä½¿ç”¨
        if desc is None:
            return jsonify({"message": "ç‰¹å¾´é‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", "status": "error"}), 400

        # ä¿å­˜å‡¦ç†ï¼ˆä¾‹: S3ã‚­ãƒ¼ã¨DBç™»éŒ²ï¼‰
        key = f"registered_images/{uuid.uuid4().hex}.jpg"
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªs3ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¨S3_BUCKETå¤‰æ•°ã‚’ä½¿ç”¨
        s3.upload_fileobj(BytesIO(response.content), S3_BUCKET, key)

        # DBã¸ã®ä¿å­˜ã¯Railså´ã§è¡Œã†ãŸã‚ã€ã“ã“ã§ã¯s3_keyã‚’è¿”ã™ã ã‘ã«ã™ã‚‹
        # with Session() as session: # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã‚’ä½¿ç”¨
        #     session.add(ProductMapping(name=name, s3_key=key))
        #     session.commit()

        return jsonify({"message": "ç™»éŒ²æˆåŠŸ", "status": "ok", "s3_key": key}), 200
    
    except requests.exceptions.RequestException as e:
        app.logger.error(f"âŒ register_image_v2 ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return jsonify({"message": f"ç”»åƒURLã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}", "status": "error"}), 400
    except Exception as e:
        app.logger.exception("âŒ register_image_v2 å¤±æ•—") # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        return jsonify({"message": "å†…éƒ¨ã‚¨ãƒ©ãƒ¼", "status": "error"}), 500


def extract_sift(pil_img_gray, dim=FEATURE_DIM):
    """preprocess_pilã§å‡¦ç†æ¸ˆã¿ã®PILã‚°ãƒ¬ãƒ¼ç”»åƒã‹ã‚‰SIFTç‰¹å¾´é‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’æŠ½å‡º"""
    if pil_img_gray.mode != "L":
        app.logger.warning(f"extract_siftã¯'L'ãƒ¢ãƒ¼ãƒ‰ã®PILç”»åƒã‚’æœŸå¾…ã—ã¾ã—ãŸãŒã€{pil_img_gray.mode}ã‚’å—ã‘å–ã‚Šã¾ã—ãŸã€‚ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚")
        pil_img_gray = pil_img_gray.convert("L")

    cv_gray_image = np.array(pil_img_gray)
    # SIFTãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´å¯èƒ½ã«ã™ã‚‹
    sift = cv2.SIFT_create(
        sigma=SIFT_SIGMA,
        # nfeatures=0,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã¾ã¾ã‹ã€èª¿æ•´ã™ã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã™ (ä¾‹: 500)
        contrastThreshold=0.03, # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ(0.04)ã‹ã‚‰èª¿æ•´ã™ã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã™
        # edgeThreshold=12 # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ(10)ã‹ã‚‰èª¿æ•´ã™ã‚‹å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚’å¤–ã™
    )
    _, des = sift.detectAndCompute(cv_gray_image, None)
    
    if des is None:
        return None
    
    vec = des.flatten()
    if vec.size == 0: # desãŒNoneã§ãªãã¦ã‚‚flattençµæœãŒç©ºã«ãªã‚‹ã‚±ãƒ¼ã‚¹å¯¾ç­–
        return None

    # å›ºå®šé•·ãƒ™ã‚¯ãƒˆãƒ«ã«ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°/ãƒˆãƒ©ãƒ³ã‚±ãƒ¼ãƒˆ
    if vec.shape[0] < dim: # &lt; ã‚’ < ã«ä¿®æ­£
        padded_vec = np.zeros(dim, dtype="float32")
        padded_vec[:vec.shape[0]] = vec
        vec = padded_vec
    else:
        vec = vec[:dim]

    norm = np.linalg.norm(vec)
    if norm == 0: # ã‚¼ãƒ­é™¤ç®—ã‚’é¿ã‘ã‚‹
        return None
    return vec / norm


# â”€â”€ å‰å‡¦ç†ãƒ˜ãƒ«ãƒ‘ãƒ¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def crop_to_object(pil_img, thresh=200): # ã“ã®é–¢æ•°ã¯ç¾åœ¨ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã›ã‚“
    arr  = np.array(pil_img.convert("RGB"))
    gray = cv2.cvtColor(arr, cv2.COLOR_RGB2GRAY)
    _, binimg = cv2.threshold(gray, thresh, 255, cv2.THRESH_BINARY_INV)
    cnts, _  = cv2.findContours(binimg, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not cnts:
        return pil_img
    x, y, w, h = cv2.boundingRect(max(cnts, key=cv2.contourArea))
    return pil_img.crop((x, y, x+w, y+h))


# def preprocess_pil(img, size=100): # ä¾‹: ç¾åœ¨ã®å€¤ã‹ã‚‰å¤§ããã—ã¦ã¿ã‚‹ (ä¾‹: 100 -> 200)
def preprocess_pil(img, size=200):
    img = ImageOps.exif_transpose(img) # EXIFæƒ…å ±ã«åŸºã¥ãå›è»¢ã‚’å…ˆã«è¡Œã†
    img = img.convert("L")
    img = img.filter(ImageFilter.MedianFilter(3))
    img = img.filter(ImageFilter.GaussianBlur(radius=1))
    img = ImageOps.fit(img, (size, size))
    return ImageOps.autocontrast(img, cutoff=1)

# â”€â”€ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ§‹ç¯‰æ©Ÿèƒ½ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def build_cache(cache_dir=CACHE_DIR, index_path=INDEX_PATH): # dimå¼•æ•°ã¯ä¸è¦ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«FEATURE_DIMã‚’ä½¿ç”¨
    os.makedirs(cache_dir, exist_ok=True)

    # 1) DB ã«ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ s3_key ã®ã¿å–å¾—
    with Session() as session:
        products = session.query(ProductMapping).all()

    s3_keys_for_index = [] # å®Ÿéš›ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ ã•ã‚ŒãŸã‚­ãƒ¼ã®ãƒªã‚¹ãƒˆ
    descriptors = []

    # 2) å„ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ â†’ å‰å‡¦ç† â†’ SIFTç‰¹å¾´é‡æŠ½å‡º
    for product in products:
        key = product.s3_key
        try:
            resp = s3.get_object(Bucket=S3_BUCKET, Key=key)
            img_pil = Image.open(BytesIO(resp["Body"].read()))
            
            processed_img_pil = preprocess_pil(img_pil) # å‰å‡¦ç†ã‚’é©ç”¨
            desc = extract_sift(processed_img_pil)      # SIFTç‰¹å¾´é‡ã‚’æŠ½å‡º

            if desc is not None:
                descriptors.append(desc)
                s3_keys_for_index.append(key)
                # å€‹åˆ¥npyãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã¯Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒä¸»ãªã‚‰ä¸è¦ã‹ã‚‚
                # sanitized_key = key.replace('/', '_') # ãƒ‘ã‚¹åŒºåˆ‡ã‚Šæ–‡å­—ã‚’ç½®æ›
                # np.save(os.path.join(cache_dir, f"{sanitized_key}.npy"), desc)
            else:
                app.logger.warning(f"âŒ ç‰¹å¾´é‡ãŒå–ã‚Œã¾ã›ã‚“ã§ã—ãŸ (build_cache): {key}")
        except Exception as e:
            app.logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ§‹ç¯‰ä¸­ã«ã‚¨ãƒ©ãƒ¼ (ç”»åƒå‡¦ç†: {key}): {e}")
            continue # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸç”»åƒã¯ã‚¹ã‚­ãƒƒãƒ—
     
    if not descriptors:
        app.logger.error("ğŸš« æœ‰åŠ¹ãªç‰¹å¾´é‡ãŒæŠ½å‡ºã•ã‚ŒãŸç”»åƒãŒ 0 ä»¶ã§ã™ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½œæˆä¸­æ­¢")
        # æ—¢å­˜ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Œã°å‰Šé™¤ã™ã‚‹
        if Path(index_path).exists():
            try:
                Path(index_path).unlink()
                app.logger.info(f"Removed existing index: {index_path}")
            except OSError as e_unlink:
                app.logger.error(f"Error removing index file {index_path}: {e_unlink}")
        if Path(KEYS_PATH).exists():
            try:
                Path(KEYS_PATH).unlink()
                app.logger.info(f"Removed existing keys file: {KEYS_PATH}")
            except OSError as e_unlink:
                app.logger.error(f"Error removing keys file {KEYS_PATH}: {e_unlink}")
        return

    xb = np.stack(descriptors).astype('float32') # Faissã®ãŸã‚ã«float32å‹ã«å¤‰æ›

    # 3) keys.json ã‚’ä¿å­˜
    with open(KEYS_PATH, "w", encoding="utf-8") as f: # s3_keys_for_index ã‚’ä¿å­˜
        json.dump(s3_keys_for_index, f, ensure_ascii=False, indent=2)

    # 4) Faiss ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼‹ä¿å­˜
    index = faiss.IndexFlatL2(FEATURE_DIM) # ã‚°ãƒ­ãƒ¼ãƒãƒ« FEATURE_DIM ã‚’ä½¿ç”¨
    index.add(xb)
    faiss.write_index(index, index_path)

    app.logger.info(f"âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥({len(s3_keys_for_index)}ä»¶) & ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆã—ã¾ã—ãŸ â†’ {cache_dir}/ , {index_path}")

# â”€â”€ ç”»åƒç™»éŒ²ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/register_image", methods=["POST"])
def register_image():
    name = request.form.get("name")
    if not name:
        return "invalid request (no name)", 400

    if "image" in request.files:
        stream = request.files["image"].stream
    elif "image_url" in request.form:
        # import requests # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ã‚ã‚‹ã®ã§ä¸è¦
        try:
            r = requests.get(request.form["image_url"])
            r.raise_for_status()
            stream = BytesIO(r.content)
        except Exception as e:
            app.logger.error(f"Failed download image_url: {e}")
            return "invalid image_url", 400
    else:
        return "invalid request (no image or image_url)", 400

    try:
        img = Image.open(stream)
        img = ImageOps.exif_transpose(img).convert("RGB")
        img.thumbnail((640, 640), Image.Resampling.LANCZOS)
        
        # S3ã‚­ãƒ¼ã®å½¢å¼ã‚’ v2 ã¨åˆã‚ã›ã‚‹ (ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ )
        # filename = f"{uuid.uuid4().hex}.jpg" # å…ƒã®å½¢å¼
        filename = f"registered_images/{uuid.uuid4().hex}.jpg" # v2ã«åˆã‚ã›ãŸå½¢å¼
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ä¿å­˜ã¯S3ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒä¸»ãªã‚‰ä¸è¦ã‹ã‚‚
        # path = os.path.join("registered_images", filename)
        # os.makedirs("registered_images", exist_ok=True)
        # img.save(path, format="JPEG", quality=80, optimize=True)
        # s3.upload_file(path, S3_BUCKET, filename, ExtraArgs={"ContentType":"image/jpeg"})
        
        # S3ã¸ç›´æ¥ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        img_byte_arr = BytesIO()
        img.save(img_byte_arr, format="JPEG", quality=80, optimize=True)
        img_byte_arr.seek(0)
        s3.upload_fileobj(img_byte_arr, S3_BUCKET, filename, ExtraArgs={"ContentType":"image/jpeg"})

        app.logger.info(f"â˜ï¸ uploaded to S3://{S3_BUCKET}/{filename}")

        # DBã¸ã®ä¿å­˜ã¯Railså´ã§è¡Œã†ãŸã‚ã€ã“ã“ã§ã¯s3_keyã‚’è¿”ã™ã ã‘ã«ã™ã‚‹
        # with Session() as session: # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã‚’ä½¿ç”¨
        #     product = ProductMapping(name=name, s3_key=filename)
        #     session.add(product)
        #     session.commit()

        return jsonify({"message": "ç™»éŒ²æˆåŠŸ", "status": "ok", "s3_key": filename}), 200 # s3_keyã‚’è¿”ã™ã‚ˆã†ã«å¤‰æ›´

    except Exception as e:
        app.logger.exception(e) # ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã‚’ãƒ­ã‚°ã«å‡ºåŠ›
        return "error", 500


# â”€â”€ ç”»åƒèªè­˜ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@app.route("/predict", methods=["POST"])
def predict():
    try:
        # 1) ç”»åƒå–å¾—
        img_stream = None
        if "image" in request.files:
            img_stream = request.files["image"].stream
        elif "image_url" in request.form:
            # import requests as req_local # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«ã‚ã‚‹ã®ã§ä¸è¦ã€ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚‚ä¸è¦
            try:
                r = requests.get(request.form["image_url"])
                r.raise_for_status()
                img_stream = BytesIO(r.content)
            except requests.exceptions.RequestException as e:
                app.logger.error(f"predictã§ã®ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
                return jsonify(error=f"ç”»åƒURLã‹ã‚‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}"), 400
        else:
            return jsonify(error="ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“"), 400

        raw_pil_img = Image.open(img_stream)
        
        # ä¸€è²«ã—ãŸå‰å‡¦ç†
        processed_pil_img = preprocess_pil(raw_pil_img)
        
        # 2) ç‰¹å¾´é‡æŠ½å‡º (çµ±ä¸€ã•ã‚ŒãŸé–¢æ•°ã‚’ä½¿ç”¨)
        q_vec = extract_sift(processed_pil_img) # dim ã¯ã‚°ãƒ­ãƒ¼ãƒãƒ« FEATURE_DIM ã‚’ä½¿ç”¨

        if q_vec is None:
            app.logger.warning("âŒ ã‚¯ã‚¨ãƒªç”»åƒã®ç‰¹å¾´é‡ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
            return jsonify(error="ç”»åƒãŒä¸æ˜ç­ã§ç‰¹å¾´é‡ã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ"), 400
        
        # Faissã¯ (n_samples, dim) ã®å½¢å¼ã®é…åˆ—ã‚’æœŸå¾…
        q_arr_expanded = np.expand_dims(q_vec.astype('float32'), 0)

        # 3) ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¨ã‚­ãƒ¼èª­ã¿è¾¼ã¿
        if not Path(INDEX_PATH).exists() or not Path(KEYS_PATH).exists():
            app.logger.error("ğŸš« Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¾ãŸã¯ã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã« /build_cache ã‚’å®Ÿè¡Œã™ã‚‹ã‹ã€ã‚¢ãƒ—ãƒªã‚’å†èµ·å‹•ã—ã¦ãã ã•ã„ã€‚")
            return jsonify(error="æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒæº–å‚™ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚"), 503 # Service Unavailable

        index = faiss.read_index(INDEX_PATH)
        with open(KEYS_PATH, "r", encoding="utf-8") as f:
            indexed_s3_keys = json.load(f) # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†…ã®ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾å¿œã™ã‚‹ã‚­ãƒ¼

        if index.ntotal == 0 or not indexed_s3_keys:
             app.logger.warning("ğŸ¤· æ¤œç´¢å¯¾è±¡ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒç©ºã§ã™ã€‚")
             return jsonify(all_similarity_scores=[]), 200

        # 4) æ¤œç´¢
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†…ã®ã‚¢ã‚¤ãƒ†ãƒ æ•°ã¾ã§ã‚’æ¤œç´¢å¯¾è±¡ã¨ã™ã‚‹
        k_search = min(len(indexed_s3_keys), index.ntotal) 
        if k_search == 0:
            return jsonify(all_similarity_scores=[]), 200
            
        D, I = index.search(q_arr_expanded, k=k_search)

        # 5) çµæœæ•´å½¢ï¼ˆé‡è¤‡åé™¤å¤–ï¼‰
        all_scores = []
        with Session() as session: # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã‚’ä½¿ç”¨
            seen_names = set()
            for dist, idx_in_index in zip(D[0], I[0]):
                if idx_in_index < 0: # FaissãŒè¿”ã™-1ã¯ç„¡åŠ¹ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹. &lt; ã‚’ < ã«ä¿®æ­£
                    continue
                
                s3_key = indexed_s3_keys[idx_in_index]
                prod = session.query(ProductMapping).filter_by(s3_key=s3_key).first()
                
                if not prod:
                    app.logger.warning(f"DBã«å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: s3_key={s3_key} (ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚ˆã‚Š)ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                    continue

                name = prod.name
                if name in seen_names:
                    continue
                seen_names.add(name)

                score = round(1.0 / (1.0 + dist), 4) if dist >= 0 else 0.0 # &gt;= ã‚’ >= ã«ä¿®æ­£
                app.logger.info(f"ğŸ“Š dist={dist:.2f}, score={score:.4f}, name={name}, s3_key={s3_key}")

                all_scores.append({
                    "name": name,
                    "score": float(score) # JSONã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã®ãŸã‚ã«floatå‹ã‚’ä¿è¨¼
                })

        # ã‚¹ã‚³ã‚¢ã§é™é †ã‚½ãƒ¼ãƒˆ
        all_scores_sorted = sorted(all_scores, key=lambda x: x["score"], reverse=True)

        return jsonify(all_similarity_scores=all_scores_sorted), 200

    except Exception as e:
        app.logger.exception("âŒ predict ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        return jsonify(error="å†…éƒ¨ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"), 500
        

# â”€â”€ ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--build-cache", action="store_true",
        help="S3 ã‹ã‚‰ç‰¹å¾´é‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼†Faissã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ"
    )
    args = parser.parse_args()

    if args.build_cache:
        build_cache() # ã‚°ãƒ­ãƒ¼ãƒãƒ« FEATURE_DIM ã‚’å†…éƒ¨ã§ä½¿ç”¨
    else:
        try:
            if not Path(INDEX_PATH).exists() or not Path(KEYS_PATH).exists():
                app.logger.info("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã®ã§è‡ªå‹•ç”Ÿæˆã—ã¾ã™ (ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«èª­ã¿è¾¼ã¿æ™‚)")
                build_cache() # ã‚°ãƒ­ãƒ¼ãƒãƒ« FEATURE_DIM ã‚’å†…éƒ¨ã§ä½¿ç”¨
        except Exception as e:
            app.logger.error(f"âŒ èµ·å‹•æ™‚ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆå¤±æ•—: {e}")

        port = int(os.environ.get("PORT", 10000))
        app.run(host="0.0.0.0", port=port, debug=False)

if __name__ == "__main__":
    main()
